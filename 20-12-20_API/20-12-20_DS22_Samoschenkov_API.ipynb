{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS-22 Самощенков Сергей\n",
    "# Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "\n",
    "# Задание 1.\n",
    "# Обязательная часть\n",
    "# Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "# Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "# KEYWORDS = ['python', 'парсинг']\n",
    "# Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "# В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "# Дополнительная часть (необязательная)\n",
    "# Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "# Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "# Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>\n",
    "\n",
    "import requests\n",
    "import time \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# определяем список хабов, которые нам интересны\n",
    "keywords = ['python', 'парсинг']\n",
    "hub_news = pd.DataFrame()\n",
    "\n",
    "# получаем страницу с самыми свежими постами\n",
    "req = requests.get('http://habr.com/ru/all/')\n",
    "soap = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "# извлекаем посты\n",
    "posts = soup.find_all('article', class_ ='post')\n",
    "\n",
    "# Вариант 1\n",
    "\n",
    "hubs = list(map(lambda x: x.find('a', class_='post__title_link'), posts))\n",
    "hub_links = list(map(lambda x: x.find('a', class_='post__title_link').get('href'), posts))\n",
    "hub_titles = list(map(lambda x: x.find('a', class_='post__title_link').getText(), posts))\n",
    "hub_dates = list(map(lambda x: x.find('span', class_='post__time').getText(), posts))\n",
    "hub_authors = list(map(lambda x: x.find('span', class_='user-info__nickname').getText(), posts))\n",
    "hub_ids = list(map(lambda x: x.parent.attrs.get('id'), posts))\n",
    "\n",
    "if len(hub_links) == len(hub_titles) == len(hub_dates) == len(hub_authors) == len(hub_ids):\n",
    "    hub_news['hub_links']=hub_links\n",
    "    hub_news['hub_titles']=hub_titles\n",
    "    hub_news['hub_dates']=hub_dates\n",
    "    hub_news['hub_authors']=hub_authors\n",
    "    hub_news['hub_ids']=hub_ids\n",
    "    hub_news = hub_news[hub_news['hub_titles'].apply(lambda x: any([keyword in x.lower() for keyword in keywords]))]\n",
    "else:\n",
    "    print ('Данный алгоритм не подходит')\n",
    "\n",
    "# Вариант 2\n",
    "hub_news2 = pd.DataFrame()\n",
    "\n",
    "for index, post in enumerate(posts):\n",
    "    # достаем id статьи\n",
    "    hub_id = post.parent.attrs.get('id')\n",
    "    # если идентификатор не найден, это что-то странное, пропускаем\n",
    "    if not hub_id:\n",
    "        continue\n",
    "    # вычленяем числовую часть номера: делим 'post_127464' на части по разделителю '_' и забираем последнюю часть\n",
    "    hub_id = int(hub_id.split('_')[-1]) \n",
    "    # достаем другие параметры статьи\n",
    "    hub_link = post.find('a', class_='post__title_link').get('href')\n",
    "    hub_title = post.find('a', class_='post__title_link').getText()\n",
    "    hub_date = post.find('span', class_='post__time').getText()\n",
    "    hub_author = post.find('span', class_='user-info__nickname').getText()\n",
    "    hub_text = ''\n",
    "    # проверяем заголовок на соответствие условиям поиска\n",
    "    if any([keyword in hub_title.lower() for keyword in keywords]):\n",
    "        # достаем полный текст статьи\n",
    "        soup = BeautifulSoup(requests.get(hub_link).text, 'html.parser')\n",
    "        time.sleep(0.2)\n",
    "        if soap.find('div', class_='post__text post__text-html post__text_v1'):\n",
    "            hub_text = soap.find('div', class_='post__text post__text-html post__text_v1').text\n",
    "        # создаем итоговый датафрейм\n",
    "        row = {'hub_title': hub_title, 'hub_date': hub_date, 'hub_author': hub_author, 'hub_link': hub_link, 'hub_id': hub_id, 'hub_text': hub_text}\n",
    "        hub_news2 = hub_news2.append(pd.DataFrame([row]))    \n",
    "\n",
    "hub_news2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                             mail      status                  date  \\\n",
       "0  saswin18178.coolteam@gmail.com  not hacked                  None   \n",
       "1                saswin@yandex.ru      hacked  2016-10-25T00:00:00Z   \n",
       "2                saswin@yandex.ru      hacked  2018-10-04T00:00:00Z   \n",
       "3                saswin@yandex.ru      hacked  2020-12-10T00:00:00Z   \n",
       "\n",
       "             site                                        description  \n",
       "0            None                                               None  \n",
       "1      tumblr.com  In May 2016, microblogging platform Tumblr rev...  \n",
       "2  autowebinar.fm  In December 2013, AutoWebinar FM's database wa...  \n",
       "3   gdepoigrat.ru  In November 2020, a collection of over 23,000 ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mail</th>\n      <th>status</th>\n      <th>date</th>\n      <th>site</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>saswin18178.coolteam@gmail.com</td>\n      <td>not hacked</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>saswin@yandex.ru</td>\n      <td>hacked</td>\n      <td>2016-10-25T00:00:00Z</td>\n      <td>tumblr.com</td>\n      <td>In May 2016, microblogging platform Tumblr rev...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saswin@yandex.ru</td>\n      <td>hacked</td>\n      <td>2018-10-04T00:00:00Z</td>\n      <td>autowebinar.fm</td>\n      <td>In December 2013, AutoWebinar FM's database wa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>saswin@yandex.ru</td>\n      <td>hacked</td>\n      <td>2020-12-10T00:00:00Z</td>\n      <td>gdepoigrat.ru</td>\n      <td>In November 2020, a collection of over 23,000 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "# Задание 2. - \n",
    "\n",
    "# Обязательная часть\n",
    "# Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "\n",
    "# mail = {'emailAddresses': [\"saswin18178.coolteam@gmail.com\"]}\n",
    "# В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "# Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\n",
    "\n",
    "import requests\n",
    "import time \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "EMAIL = ['saswin18178.coolteam@gmail.com', 'saswin@yandex.ru']\n",
    "\n",
    "req_url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = {\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Content-Length': '45',\n",
    "    'Content-Type': 'application/json;charset=UTF-8',\n",
    "    'Host': 'identityprotection.avast.com',\n",
    "    'Origin': 'https://www.avast.com',\n",
    "    'Referer': 'https://www.avast.com/',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"87\", \" Not;A Brand\";v=\"99\", \"Chromium\";v=\"87\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Site': 'same-site',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',\n",
    "    'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "    'Vaar-Version': '0',\n",
    "}\n",
    "\n",
    "def find_breaches(mail):\n",
    "    \"\"\"\n",
    "    Возвращает датафрейм со списком взломов для заданного ящика\n",
    "    Поля датафрейма: 'mail', 'status' = hacked|not hacked, 'date', 'site', 'description'\n",
    "    \"\"\"\n",
    "    time.sleep(0.3)\n",
    "    json_mail = {'emailAddresses': [mail]}\n",
    "    req = requests.post(req_url, json = json_mail, headers = headers)    \n",
    "    req_df = pd.DataFrame(req.json())\n",
    "\n",
    "    breaches_count = len(req_df.index)//2\n",
    "    mail_breaches_df = pd.DataFrame(columns=['mail', 'status', 'date', 'site', 'description'])  \n",
    "    \n",
    "    if breaches_count == 0:\n",
    "        row = {'mail': mail, 'status': 'not hacked', 'date' : None, 'site' : None, 'description' : None}\n",
    "        mail_breaches_df = mail_breaches_df.append(pd.DataFrame([row]))  \n",
    "    else: \n",
    "        for i in range(breaches_count):\n",
    "            br = req_df.iloc[i]['breaches']\n",
    "            publishDate = br['publishDate']\n",
    "            site = br['site']\n",
    "            description = br['description']\n",
    "            row = {'mail': mail, 'status': 'hacked', 'date' : publishDate, 'site' : site, 'description' : description}\n",
    "            mail_breaches_df = mail_breaches_df.append(pd.DataFrame([row]))\n",
    "    \n",
    "    return mail_breaches_df\n",
    "\n",
    "breaches_df = pd.DataFrame(columns=['mail', 'status', 'date', 'site', 'description'])\n",
    "\n",
    "for mail in EMAIL:\n",
    "    mail_breaches_df = find_breaches(mail)\n",
    "    breaches_df = pd.concat([breaches_df, mail_breaches_df], ignore_index=True)\n",
    "\n",
    "breaches_df.head(35)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополнительная часть (необязательная)\n",
    "# Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "# Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.get\n",
    "\n",
    "# GROUP = 'netology'  \n",
    "# TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "# В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     'target_type': 'posts',\n",
    "#     'q': {keywords[0]},\n",
    "#     'order_by': 'date'\n",
    "# }\n",
    "# URL = 'https://habr.com/ru/search/'\n",
    "# headers = {}\n",
    "\n",
    "# time.sleep(0.2)\n",
    "\n",
    "# req = requests.get(URL, params)\n",
    "# req = requests.post(URL, params, headers)\n",
    "# req.text\n",
    "\n",
    "# soup = BeautifulSoup(req.text, 'html.parser')\n",
    "# posts = soup.find_all('article', class_='post')\n",
    "\n",
    "# for post in posts:\n",
    "#     post_id = post.parent.attrs.get('id')\n",
    "#     if not post_id:\n",
    "#         continue\n",
    "#     post_id = int(post_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}